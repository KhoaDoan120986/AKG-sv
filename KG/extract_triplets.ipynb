{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932cf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652bde3",
   "metadata": {},
   "source": [
    "## Get Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07353721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = 'msvd'\n",
    "dataset = 'msrvtt'\n",
    "\n",
    "def msvd_load_captions(caption_fpath):\n",
    "    df = pd.read_csv(caption_fpath)\n",
    "    df = df[df['Language'] == 'English']\n",
    "    df = df[pd.notnull(df['Description'])]\n",
    "    captions = df['Description'].values\n",
    "    return captions\n",
    "\n",
    "if dataset == 'msvd':\n",
    "    cnt = 0\n",
    "    train_id_path = '../MSVD/metadata/train.list'\n",
    "    train_sentence_path = '../MSVD/metadata/train.csv'\n",
    "    sentence_path = '../MSVD/metadata/msvd_sentence.txt'\n",
    "    w = open(sentence_path, 'a')\n",
    "    sentence = msvd_load_captions(train_sentence_path)\n",
    "    print(sentence.size)\n",
    "    for i in range(sentence.size):\n",
    "        w.writelines(sentence[i])\n",
    "        w.write('\\n')\n",
    "        cnt += 1\n",
    "    w.close()\n",
    "    print(cnt)\n",
    "\n",
    "if dataset == 'msrvtt':\n",
    "    file_path = '../MSR-VTT/metadata/train.json'\n",
    "    sentence_path = '../MSR-VTT/metadata/msrvtt_sentence.txt'\n",
    "    list1 = []\n",
    "    cnt = 0\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        load_data = json.load(f)\n",
    "        f.close()\n",
    "    w = open(sentence_path, 'a')\n",
    "    # print(load_data, type(load_data))\n",
    "    for i in load_data.keys():\n",
    "        list1.append(load_data[i])\n",
    "    for j in range(len(list1)):\n",
    "        for key in list1[j]:\n",
    "            w.writelines(list1[j][key])\n",
    "            w.write('\\n')\n",
    "            cnt += 1\n",
    "            print(list1[j][key])\n",
    "    w.close()\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36045b",
   "metadata": {},
   "source": [
    "##  Extract triples from sentences using Standford NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27477f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('stanford-corenlp-4.2.2', lang='en')\n",
    "\n",
    "dataset = 'msvd'\n",
    "sentence_path = dataset + 'sentence.txt'\n",
    "entity_rel_path = dataset + 'entity_total.txt'\n",
    "log_path = 'error_log.txt'\n",
    "\n",
    "f = open(sentence_path, 'r')\n",
    "w = open(entity_rel_path, 'w')\n",
    "log = open(log_path, 'w')\n",
    "lines = f.readlines()\n",
    "cnt = 0\n",
    "num_rel = 0\n",
    "\n",
    "for line in tqdm(lines):\n",
    "    sentence = line.strip()\n",
    "    try:\n",
    "        output = nlp.annotate(sentence, properties={\n",
    "            \"annotators\": \"tokenize,lemma,ssplit,pos,depparse,natlog,openie\",\n",
    "            \"outputFormat\": \"json\",\n",
    "            'openie.triple.strict': 'true',\n",
    "            'openie.max_entailments_per_clause': '1'\n",
    "        })\n",
    "\n",
    "        data = json.loads(output)\n",
    "        for i in range(len(data['sentences'])):\n",
    "            result = [data[\"sentences\"][i][\"openie\"]]\n",
    "            lemmas = data[\"sentences\"][i][\"tokens\"]\n",
    "            cnt += 1\n",
    "\n",
    "            for g in result:\n",
    "                for rel in g:\n",
    "                    subj_start, subj_end = rel['subjectSpan']\n",
    "                    obj_start, obj_end = rel['objectSpan']\n",
    "                    rel_start, rel_end = rel['relationSpan']\n",
    "\n",
    "                    if max(subj_end, obj_end, rel_end) > len(lemmas):\n",
    "                        log.write(f\"[Warning] Span out of range at sentence {cnt}: {sentence}\\n\")\n",
    "                        continue\n",
    "\n",
    "                    # Sử dụng join thay vì nối chuỗi ngược\n",
    "                    l_subject = ' '.join([token['lemma'] for token in lemmas[subj_start:subj_end]])\n",
    "                    l_object = ' '.join([token['lemma'] for token in lemmas[obj_start:obj_end]])\n",
    "                    # l_relation = ' '.join([token['lemma'] for token in lemmas[rel_start:rel_end]])\n",
    "\n",
    "                    subj_tokens = [\n",
    "                        token['lemma'] for token in lemmas[subj_start:subj_end]\n",
    "                        if token['pos'].startswith('NN')\n",
    "                    ]\n",
    "                    l_subject = ' '.join(subj_tokens)\n",
    "\n",
    "                    obj_tokens = [\n",
    "                        token['lemma'] for token in lemmas[obj_start:obj_end]\n",
    "                        if token['pos'].startswith('NN')\n",
    "                    ]\n",
    "                    l_object = ' '.join(obj_tokens)\n",
    "\n",
    "                    relation_tokens = [\n",
    "                        token['lemma'] for token in lemmas[rel_start:rel_end]\n",
    "                        if token['pos'].startswith('VB')\n",
    "                    ]\n",
    "                    l_relation = ' '.join(relation_tokens)\n",
    "\n",
    "                    if not l_subject or not l_object or not l_relation:\n",
    "                        continue\n",
    "\n",
    "                    relationSent = f\"{l_subject.strip()} &{l_object.strip()} &{l_relation.strip()}\"\n",
    "                    w.write(relationSent + '\\n')\n",
    "                    num_rel += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        log.write(f\"[Error] Exception at sentence {cnt}: {sentence}\\n\")\n",
    "        log.write(f\"         Error message: {str(e)}\\n\")\n",
    "w.close()\n",
    "f.close()\n",
    "log.close()\n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2622022",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Result written to:', entity_rel_path)\n",
    "print('Error log written to:', log_path)\n",
    "print('Total number of processed sentences: ' + str(cnt))\n",
    "print('Total number of relations extracted: ' + str(num_rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a83dc9",
   "metadata": {},
   "source": [
    "## Filter triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b7e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = set(words.words())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "akg_vocab = None # replace vocabulary of model\n",
    "\n",
    "def load_triples(filepath):\n",
    "    triples = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('&')\n",
    "            if len(parts) == 3:\n",
    "                head, tail, relation = parts\n",
    "                triples.append((head.strip(), tail.strip(), relation.strip()))\n",
    "    return triples\n",
    "\n",
    "def is_valid_entity(entity):\n",
    "    tokens = entity.split()\n",
    "    for token in tokens:\n",
    "        if token in akg_vocab:\n",
    "            continue\n",
    "        if len(token) == 1 or not token.isalpha() or token not in english_vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def filter_triples(triples):\n",
    "    remove_relations = {\"be\", \"have\", \"do\", \"get\", \"can\", \"is\", \"are\"}\n",
    "    filtered = []\n",
    "    for h, r, t in triples:\n",
    "        if r in remove_relations:\n",
    "            continue\n",
    "        if len(h.split()) < 1 or len(t.split()) < 1:\n",
    "            continue\n",
    "        if len(h.split()) > 2 or len(t.split()) > 2:\n",
    "            continue\n",
    "        if not is_valid_entity(h) or not is_valid_entity(t):\n",
    "            continue\n",
    "        filtered.append((h, r, t))\n",
    "    return filtered\n",
    "\n",
    "def normalize_triples(triples):\n",
    "    def normalize_text(text):\n",
    "        quantifiers = {\n",
    "            \"some\", \"many\", \"several\", \"few\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\",\n",
    "            \"all\", \"both\", \"each\", \"every\", \"no\", \"none\", \"any\", \"most\", \"more\", \"less\"\n",
    "        }\n",
    "        quantifiers.update(str(i) for i in range(1, 51))\n",
    "        text = text.lower()\n",
    "        text = re.sub(rf\"[{string.punctuation}]\", \"\", text)\n",
    "        tokens = text.strip().split()\n",
    "        tokens = [w for w in tokens if w not in stop_words.union(quantifiers)]\n",
    "        return ' '.join(tokens)\n",
    "    norm_triples = []\n",
    "    for h, r, t in triples:\n",
    "        norm_h = normalize_text(h)\n",
    "        norm_t = normalize_text(t)\n",
    "        norm_r = normalize_text(r)\n",
    "        if norm_h and norm_t and norm_r:\n",
    "            norm_triples.append((norm_h, norm_r, norm_t))\n",
    "    return norm_triples\n",
    "\n",
    "def deduplicate_triples(triples):\n",
    "    return list(set(triples))\n",
    "\n",
    "triples = load_triples(dataset + \"_entity_total.txt\")\n",
    "triples = normalize_triples(triples)\n",
    "triples = filter_triples(triples)\n",
    "triples = deduplicate_triples(triples)\n",
    "\n",
    "with open(dataset + \"_filtered_entities.txt\", \"w\") as f:\n",
    "    for h, t, r in triples:\n",
    "        relationSent = f\"{h} &{t} &{r}\"\n",
    "        f.write(relationSent + '\\n')\n",
    "print('number of triples:', len(triples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a68f7a",
   "metadata": {},
   "source": [
    "## Build dataset for TransE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def generate_mappings(triples):\n",
    "    entity2id, relation2id = {}, {}\n",
    "    eid, rid = 0, 0\n",
    "    for h, r, t in triples:\n",
    "        for ent in [h, t]:\n",
    "            if ent not in entity2id:\n",
    "                entity2id[ent] = eid\n",
    "                eid += 1\n",
    "        if r not in relation2id:\n",
    "            relation2id[r] = rid\n",
    "            rid += 1\n",
    "    return entity2id, relation2id\n",
    "\n",
    "def split_data(triples, val_ratio=0.1, test_ratio=0.1):\n",
    "    random.seed(42) \n",
    "    random.shuffle(triples)\n",
    "    n = len(triples)\n",
    "    test_size = int(n * test_ratio)\n",
    "    val_size = int(n * val_ratio)\n",
    "    test_triples = triples[:test_size]\n",
    "    val_triples = triples[test_size:test_size+val_size]\n",
    "    train_triples = triples[test_size+val_size:]\n",
    "    return train_triples, val_triples, test_triples\n",
    "\n",
    "def write_output(train_triples, val_triples, test_triples, entity2id, relation2id, path):\n",
    "    files = [\n",
    "        'train2id.txt',\n",
    "        'valid2id.txt',\n",
    "        'test2id.txt',\n",
    "        'entity2id.txt',\n",
    "        'relation2id.txt'\n",
    "    ]\n",
    "    \n",
    "    # Xóa file cũ nếu tồn tại\n",
    "    for file in files:\n",
    "        file_path = os.path.join(path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "\n",
    "    # Train\n",
    "    with open(os.path.join(path, 'train2id.txt'), 'w') as f:\n",
    "        f.write(f\"{len(train_triples)}\\n\")\n",
    "        for h, r, t in train_triples:\n",
    "            f.write(f\"{entity2id[h]}\\t{entity2id[t]}\\t{relation2id[r]}\\n\")\n",
    "\n",
    "    # Validation\n",
    "    with open(os.path.join(path, 'valid2id.txt'), 'w') as f:\n",
    "        f.write(f\"{len(val_triples)}\\n\")\n",
    "        for h, r, t in val_triples:\n",
    "            f.write(f\"{entity2id[h]}\\t{entity2id[t]}\\t{relation2id[r]}\\n\")\n",
    "\n",
    "    # Test\n",
    "    with open(os.path.join(path, 'test2id.txt'), 'w') as f:\n",
    "        f.write(f\"{len(test_triples)}\\n\")\n",
    "        for h, r, t in test_triples:\n",
    "            f.write(f\"{entity2id[h]}\\t{entity2id[t]}\\t{relation2id[r]}\\n\")\n",
    "\n",
    "    # Entities\n",
    "    with open(os.path.join(path, 'entity2id.txt'), 'w') as f:\n",
    "        f.write(f\"{len(entity2id)}\\n\")\n",
    "        for e, i in entity2id.items():\n",
    "            f.write(f\"{e}\\t{i}\\n\")\n",
    "\n",
    "    # Relations\n",
    "    with open(os.path.join(path, 'relation2id.txt'), 'w') as f:\n",
    "        f.write(f\"{len(relation2id)}\\n\")\n",
    "        for r, i in relation2id.items():\n",
    "            f.write(f\"{r}\\t{i}\\n\")\n",
    "\n",
    "entity2id, relation2id = generate_mappings(triples)\n",
    "train_triples, val_triples, test_triples = split_data(triples)\n",
    "\n",
    "print(f\"Number of entities: {len(entity2id)}\")\n",
    "print(f\"Number of relations: {len(relation2id)}\")\n",
    "print(f\"Train triples: {len(train_triples)}\")\n",
    "print(f\"Validation triples: {len(val_triples)}\")\n",
    "print(f\"Test triples: {len(test_triples)}\")\n",
    "\n",
    "write_output(train_triples, val_triples, test_triples, entity2id, relation2id, path=f\"./benchmarks/{dataset}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
